# Ø¨ÙˆØª ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù…ÙƒØªØ¨Ø© Ù…ØªÙˆÙØ±Ø©
# ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©ØŒ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ ÙˆØ¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø¯Ù… ÙˆØ§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
# ÙŠØ®ØªØ§Ø± ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨ÙŠÙ† Whisper, Vosk, Ø£Ùˆ Google SpeechRecognition

from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
from pydub import AudioSegment
import os
import math
import time

# Ø§Ù„ØªÙˆÙƒÙ†
TOKEN = os.environ.get("TOKEN", "8584666863:AAHZ3xApgMsvioTzkd7BoIed38z5VKCSYaE")

MAX_MESSAGE_LENGTH = 3500
CHUNK_LENGTH_MS = 60_000

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨
USE_WHISPER = False
USE_VOSK = False
USE_SPEECHREC = False

try:
    import whisper
    USE_WHISPER = True
except:
    try:
        from vosk import Model, KaldiRecognizer
        USE_VOSK = True
    except:
        try:
            import speech_recognition as sr
            USE_SPEECHREC = True
        except:
            pass

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ù‹Ø§
if USE_WHISPER:
    model = whisper.load_model("small")

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Vosk Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ù‹Ø§
if USE_VOSK:
    if not os.path.exists("vosk-model"):
        print("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…ÙˆØ°Ø¬ VoskØŒ ÙŠØ¬Ø¨ ØªØ­Ù…ÙŠÙ„Ù‡ ÙŠØ¯ÙˆÙŠÙ‹Ø§")
    else:
        vosk_model = Model("vosk-model")

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("ğŸ™ï¸ Ù…Ø±Ø­Ø¨Ù‹Ø§! Ø£Ø±Ø³Ù„ Ø±Ø³Ø§Ù„Ø© ØµÙˆØªÙŠØ© Ù„Ø£Ø­ÙˆÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ù†Øµ")

async def send_long_text(message, text):
    for i in range(0, len(text), MAX_MESSAGE_LENGTH):
        await message.reply_text(text[i:i + MAX_MESSAGE_LENGTH])

async def speech_to_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    message = update.message

    if not (message.voice or message.audio):
        await message.reply_text("âš ï¸ Ø£Ø±Ø³Ù„ Ø±Ø³Ø§Ù„Ø© ØµÙˆØªÙŠØ© Ø£Ùˆ Ù…Ù„Ù ØµÙˆØªÙŠ ÙÙ‚Ø·")
        return

    file = await (message.voice.get_file() if message.voice else message.audio.get_file())
    input_path = "input_audio"
    wav_path = "full_audio.wav"
    await file.download_to_drive(input_path)

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ WAV
    sound = AudioSegment.from_file(input_path)
    sound = sound.set_channels(1).set_frame_rate(16000)
    sound.export(wav_path, format="wav")

    full_text = ""
    chunks = math.ceil(len(sound) / CHUNK_LENGTH_MS)
    start_time = time.time()

    progress_message = await message.reply_text(
        f"â³ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...\nğŸ”„ Ø§Ù„ØªÙ‚Ø¯Ù…: 0% (0 / {chunks})\nâ±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ: --"
    )

    for i in range(chunks):
        start_ms = i * CHUNK_LENGTH_MS
        end_ms = min((i + 1) * CHUNK_LENGTH_MS, len(sound))
        chunk = sound[start_ms:end_ms]

        chunk_path = f"chunk_{i}.wav"
        chunk.export(chunk_path, format="wav")

        text = ""
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ Ù…ÙƒØªØ¨Ø© Ù…ØªÙˆÙØ±Ø©
        if USE_WHISPER:
            result = model.transcribe(chunk_path, language="auto", fp16=False)
            text = result['text'].strip()
        elif USE_VOSK:
            import wave, json
            wf = wave.open(chunk_path, "rb")
            rec = KaldiRecognizer(vosk_model, wf.getframerate())
            data = wf.readframes(wf.getnframes())
            if rec.AcceptWaveform(data):
                res = json.loads(rec.Result())
                text = res.get('text','')
            wf.close()
        elif USE_SPEECHREC:
            import speech_recognition as sr
            r = sr.Recognizer()
            with sr.AudioFile(chunk_path) as source:
                audio_data = r.record(source)
                try:
                    text = r.recognize_google(audio_data, language="ar-AR")
                except:
                    try:
                        text = r.recognize_google(audio_data, language="en-US")
                    except:
                        text = ""

        if text:
            full_text += text + "\n"

        os.remove(chunk_path)

        elapsed = time.time() - start_time
        completed = i + 1
        avg_time_per_chunk = elapsed / completed
        remaining_chunks = chunks - completed
        remaining_seconds = int(avg_time_per_chunk * remaining_chunks)
        minutes = remaining_seconds // 60
        seconds = remaining_seconds % 60
        percent = int((completed / chunks) * 100)

        await progress_message.edit_text(
            f"â³ Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª...\n"
            f"ğŸ”„ Ø§Ù„ØªÙ‚Ø¯Ù…: {percent}% ({completed} / {chunks})\n"
            f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ: {minutes} Ø¯Ù‚ÙŠÙ‚Ø© {seconds} Ø«Ø§Ù†ÙŠØ©"
        )

    if os.path.exists(input_path): os.remove(input_path)
    if os.path.exists(wav_path): os.remove(wav_path)

    if not full_text.strip():
        await progress_message.edit_text("âš ï¸ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ ÙˆØ§Ø¶Ø­ Ù…Ù† Ø§Ù„ØµÙˆØª")
        return

    await progress_message.edit_text("âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø³Ø¨Ø© 100%")
    header = "ğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ:\n\n"
    await send_long_text(message, header + full_text.strip())

if __name__ == '__main__':
    app = ApplicationBuilder().token(TOKEN).build()
    app.add_handler(CommandHandler('start', start))
    app.add_handler(MessageHandler(filters.VOICE | filters.AUDIO, speech_to_text))

    print("ğŸ‰ Adaptive Speech to Text Bot is running!")
    app.run_polling()
